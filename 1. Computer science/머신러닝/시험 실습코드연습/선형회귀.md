# linear regression (단일 선형회귀)

``` python
# dataset 정규화
x_mean = np.mean(x)
y_mean = np.mean(y)
x_std = np.std(x)
y_std = np.std(y)

#정규화
x = (x-x_mean)/x_std
y = (y-y_mean)/y_std
```

```python
class LinearRegression():

    def __init__(self, learning_rate=0.001, n_iters=1000):
        # init
        self.lr=learning_rate
        self.n_iters=n_iters
        self.weight=None
        self.bias=None

    def fit(self, x, y):
        # Init weight
        self.weight=np.zeros(1)
        self.bias=0
        n_samples,_=x.shape

		for i in range(n_iters):
		   # y햇 먼저정의
		   y_pred=np.dot(x,self.weight)+self.bias

		   # 편미분으로 각각 기울기 계산
           dw=(1/n_samples)*np.sum(np.dot((y_pred-y),x))
           db=(1/n_samples)*np.sum(y_pred-y)
           
		   # 경사하강법 각각의 weight, bias 에 적용
		   self.weight=self.weight-(self.lr*dw)
		   self.bias=self.weight-(self.lr*db)

    def predict(self, x):
		 y_pred=np.dot(x,self.weight)+self.bias

		 return y_pred
   
```

``` python
model=LinearRegression()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)

loss=np.sum((y-y_pred)**2)/len(y_test)
```


# linear regression (다중 선형회귀)

``` python 
# 정규화
x[:,0] = (x[:,0]-np.mean(x[:,0]))/np.std(x[:,0])
x[:,1] = (x[:,1]-np.mean(x[:,1]))/np.std(x[:,1])
y = (y-np.mean(y))/np.std(y)


```