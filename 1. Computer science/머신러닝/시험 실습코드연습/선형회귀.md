---
tags: ComputerScience, 머신러닝
---
# linear regression (단일 선형회귀)

``` python
# dataset 정규화
x_mean = np.mean(x)
y_mean = np.mean(y)
x_std = np.std(x)
y_std = np.std(y)

#정규화
x = (x-x_mean)/x_std
y = (y-y_mean)/y_std
```

```python
class LinearRegression():

    def __init__(self, learning_rate=0.001, n_iters=1000):
        # init
        self.lr=learning_rate
        self.n_iters=n_iters
        self.weight=None
        self.bias=None

    def fit(self, x, y):
        # Init weight
        self.weight=np.zeros(1)
        self.bias=0
        n_samples,_=x.shape

		for i in range(n_iters):
		   # y햇 먼저정의
		   y_pred=np.dot(x,self.weight)+self.bias

		   # 편미분으로 각각 기울기 계산
           dw=(1/n_samples)*np.sum(np.dot((y_pred-y),x))
           db=(1/n_samples)*np.sum(y_pred-y)
           
		   # 경사하강법 각각의 weight, bias 에 적용
		   self.weight=self.weight-(self.lr*dw)
		   self.bias=self.weight-(self.lr*db)

    def predict(self, x):
		 y_pred=np.dot(x,self.weight)+self.bias

		 return y_pred
   
```

``` python
model=LinearRegression()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)

loss=np.sum((y-y_pred)**2)/len(y_test)
```


# linear regression (다중 선형회귀)

``` python 
# 정규화
x[:,0] = (x[:,0]-np.mean(x[:,0]))/np.std(x[:,0])
x[:,1] = (x[:,1]-np.mean(x[:,1]))/np.std(x[:,1])
y = (y-np.mean(y))/np.std(y)


```

``` python
class MutipleRegression():

    def __init__(self):
		self.theta=None
  

    def fit(self, x, y):
     n_samples,n_features=x.shape
     x = np.concatenate((x,np.expand_dims(np.ones(x.shape[0]),1)),1)
     self.theta = np.dot(np.linalg.inv(np.dot(x.T,x)),np.dot(x.T,y))
  

    def predict(self, x):
      # Prediction
		 x=np.concatenate((x,np.expand_dims(np.ones(x.shape[0]),1)),1)
		 y=np.dot(x.self.theta)
		return y
     
```


--------------------------

# 두번째 빽빽이

# 단일 선형회귀

``` python
# 정규화
x_mean=np.mean(x)
y_mean=np.mean(y)
x_std=np.std(x)
y_std=np.std(y)

x=(x-x_mean)/x_std
y=(y-y_mean)/y-std
```

```python
class LinearRegression():
    def __init__(self, learning_rate=0.001, n_iters=1000):
    self.lr=learning_rate
    self.n_iters=n_iters
    self.weight=None
    self.bias=None

    def fit(self, x, y):
      self.weight=np.zeros(1)
      self.bias=0
      n_samples,_=x.shape
      
      for i in range(self.n_iters):
           y_pred=np.dot(x,self.weight)+self.bias

           dw=(1/n_samples)*(np.sum(np.dot((y_pred-y),x))
           db=(1/n_samples)*(np.sum(y_pred-y))

           self.weight=self.weight-self.lr*dw
           self.bias=self.bias-self.lr*db
       
    def predict(self, x):
         y_pred=np.dot(x,self.weight)+self.bias
		 return y_pred
  
```

``` python
# loss 계산식
loss=np.sum((y.test-y_pred)**2)/len(y_test)
```


# 다중 선형회귀

``` python
# 정규화
x[:,0]=(x[:,0]-np.mean(x[:,0]))/np.std(x[:,0])
x[:,1]=(x[:,0]-np.mean(x[:,1]))/np.std(x[:,1])
y=(y-np.mean(y))/np.std(y)

```

``` python
class MutipleRegression():

    def __init__(self):
        # init
        self.theta=None

    def fit(self, x, y):
        # Init weight
		n_samples,n_features=x.shape
		x=np.concatenate((x,np.expand_dims(np.ones(x.shape(0)),1)),1)
		self.theta=np.dot(np.linalg.inv(np.dot(x.T,x)),np.dot(x.T,y))	

    def predict(self, x):
      # Prediction
		x=np.concatenate((x,np.expand_dims(np.ones(x.shape(0)),1)),1)
		y=np.dot(x,self.theta)
		return y

        #Ordinary Least Squre Method
```

-----------------

# 세번째 빽빽이

# single

``` python
x=(x-np.mean(x))/np.std(x)
y=(y-np.mean(y))/np.std(y)
```

``` python
class LinearRegression():

    def __init__(self, learning_rate=0.001, n_iters=1000):
    self.lr=learning_rate
    self.n_iters=n_iters
	self.weight=None
	self.bias=None

    def fit(self, x, y):
		self.weight=np.zeros(1)
		self.bias=0
		n_samples,_=x.shape

		for i in range(n_iters):
		    y_pred=np.dot(x,self.weight)+self.bias

            dw=(1/n_samples)*np.sum(np.dot((y_pred-y),x)
            db=(1/n_samples)*np.sum(y_pred-y)

			self.weight=self.weight-lr*dw
			self.bias=self.bias-lr*db
  
    def predict(self, x):

      # Prediction
	    y_pred=np.dot(x,self.weight)+self.bias
  

      return y_pred
```



# 다중

``` python
#정규화
x[:,0]=(x-np.mean(x[:,0]))/np.std(x[:0])
x[:,1]=(x-np.mean(x[:,1]))/np.std(x[:1])
y=(y-np.mean(y))/np.std(y)
```

``` python
class MutipleRegression():

    def __init__(self):
        self.theta=None
   

    def fit(self, x, y):
		n_samples,n_features=x.shape
		x=np.concatenate((x,np.expand_dims(np.ones(x.shape))))


    def predict(self, x):

```